Web scraping, or web harvesting, is done in order to extract and collect data from websites. Web scraping comes in handy in terms of model development, which requires data to be collected on the fly that's true, relevant to the topic, and accurate. This is desirable as it takes less time compared to implementing datasets. The data that's collected is stored in various formats, such as JSON, CSV, XML, and more, is written to databases for later use, and is also made available online as datasets. 

#### Managing scraped data
In this section, we will explore some tools and learn more about handling and managing the data that we have scraped or extracted from certain websites. 

Data that's collected from websites using scraping scripts is known as raw data. This data might require some additional tasks to be performed on top of it before it can be processed further so that we can gain an insight on it. Therefore, raw data should be verified and processed (if required), which can be done by doing the following:

**Cleaning:** As the name suggests, this step is used to remove unwanted pieces of information, such as space and whitespace characters, and unwanted portions of text. 