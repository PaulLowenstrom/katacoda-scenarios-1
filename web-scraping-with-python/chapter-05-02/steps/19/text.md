Click the listed Spider, and detailed information and available options will be displayed as shown in the following screenshot:

![](https://github.com/fenago/katacoda-scenarios/raw/master/web-scraping-with-python/chapter-05-02/steps/19/1.png)

Click RUN to start crawling the chosen Spider as seen here:

![](https://github.com/fenago/katacoda-scenarios/raw/master/web-scraping-with-python/chapter-05-02/steps/19/2.png)

Click RUN with the default options.

Crawling jobs will be listed as seen in the following screenshot. We can browse through the Completed jobs for details on Items, Requests, Errors, Logs, and so on:

![](https://github.com/fenago/katacoda-scenarios/raw/master/web-scraping-with-python/chapter-05-02/steps/19/3.png)

When exploring items for completed jobs, options such as filters, data export, and downloading with crawling job details for requests, logs, stats, and so on are available in the job details. More information can be loaded by clicking a particular Spider listed:

![](https://github.com/fenago/katacoda-scenarios/raw/master/web-scraping-with-python/chapter-05-02/steps/19/4.png)

Using the actions listed previously, we can deploy Scrapy Spider successfully using the Scraping hub.

In this section, we used and explored the Scraping hub to deploy the Scrapy Spider.