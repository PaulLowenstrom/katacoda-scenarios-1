Spark SQL Architecture
The Spark SQL Architecture consists of the following components.

DataSource API	The Data Source API is the universalAPI to load and store structured data. This is similar to the textFile, binaryFile, Sequence File APIs in Spark core (RDD). Instead of so many different APIs for different formats of data, we have Data Source API which can load and store structured data. Data Source API has has built-in support for JDBC, Hive, Avro, JSON, Parquet, etc. Data Source API can automatically infer schema without the user explicitly mentioning the schema. We can also specify the schema using Data Source API
Data Frames	Data Frames are like advanced version of RDDs. Data Frames are distributed collection of data represented in the form of rows and named columns. All the features of RDDs also apply to Data Frames. They are distributed, lazy, can be cached and are immutable. In other words, Data Frames are similar to that of tables in RDBMS but with more advanced capabilities. Since Data Frames are similar to that of RDBMS tables, we can simply run SQL like queries on our Data Frames, and have the data processed on our Spark cluster in distributed manner. 
SQL Interpreter & Optimizer	The queries on Data Frames are run in SQL which is a high level language. So, we need a SQL Interpreter or a SQL Parser which will interpret or parse our SQL queries.  The Optimizer in Spark SQL is called Catalyst. The Catalyst optimizer works on the SQL Data Structure trees and transforms the logical plan to an optimized plan, which inturn will be transformed to a physical plan. In simple terms, this component helps us processes our big data in an efficient and optimized way.

Please check the link in References section to learn more about SQL’s Catalyst Optimizer.
Spark SQL Thrift Server	The Spark SQL Thrift server is used as an abstraction layer to connect Spark SQL with various Business Intelligence (BI) tools. The Spark Thrift Server is similar to that of Hive Thrift Server. So, instead of running queries from BI tools via Hive Thrift server as Map Reduce jobs, we can use the Spark Thrift Server and use Spark’s features. Since Spark is faster than Hadoop Map Reduce, we can have our queries processes faster in an efficient manner. The BI tools can be connected with Spark using the JDBC or ODBC drivers. For example, Tableau can be connected to Spark SQL using the ODBC driver and then run the queries from Tableau on Spark.
Tungsten	Tungsten is a Spark SQL component which helps in memory tuning and optimization of Spark jobs. Tungsten’s memory management is developed to address the drawbacks in JVM’s memory management. With Catalyst and Tungsten the Spark SQL jobs are much faster and efficient when compared to RDDs.
