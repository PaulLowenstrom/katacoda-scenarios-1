{
  "nbformat_minor": 1, 
  "nbformat": 4, 
  "cells": [
    {
      "source": [
        "# Perceptron Algorithm on the Sonar Dataset\n", 
        "from random import seed\n", 
        "from random import randrange\n", 
        "from csv import reader\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "In this section, we will train a Perceptron model using stochastic gradient descent on the\n", 
        "Sonar dataset. The example assumes that a CSV copy of the dataset is in the current working\n", 
        "directory with the file name sonar.all-data.csv.<br />\n", 
        "\n", 
        "The dataset is first loaded, the string values\n", 
        "converted to numeric and the output column is converted from strings to the integer values\n", 
        "of 0 to 1. This is achieved with helper functions load csv(), str column to float() and\n", 
        "str column to int() to load and prepare the dataset."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# Load a CSV file\n", 
        "def load_csv(filename):\n", 
        "\tdataset = list()\n", 
        "\twith open(filename, 'r') as file:\n", 
        "\t\tcsv_reader = reader(file)\n", 
        "\t\tfor row in csv_reader:\n", 
        "\t\t\tif not row:\n", 
        "\t\t\t\tcontinue\n", 
        "\t\t\tdataset.append(row)\n", 
        "\treturn dataset\n", 
        "\n", 
        "# Convert string column to float\n", 
        "def str_column_to_float(dataset, column):\n", 
        "\tfor row in dataset:\n", 
        "\t\trow[column] = float(row[column].strip())\n", 
        "\n", 
        "# Convert string column to integer\n", 
        "def str_column_to_int(dataset, column):\n", 
        "\tclass_values = [row[column] for row in dataset]\n", 
        "\tunique = set(class_values)\n", 
        "\tlookup = dict()\n", 
        "\tfor i, value in enumerate(unique):\n", 
        "\t\tlookup[value] = i\n", 
        "\tfor row in dataset:\n", 
        "\t\trow[column] = lookup[row[column]]\n", 
        "\treturn lookup\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "We will use k-fold cross-validation to estimate the performance of the learned model on\n", 
        "unseen data. This means that we will construct and evaluate k models and estimate the\n", 
        "performance as the mean model error. Classification accuracy will be used to evaluate each\n", 
        "model. These behaviors are provided in the cross validation split(), accuracy metric()\n", 
        "and evaluate algorithm() helper functions."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# Split a dataset into k folds\n", 
        "def cross_validation_split(dataset, n_folds):\n", 
        "\tdataset_split = list()\n", 
        "\tdataset_copy = list(dataset)\n", 
        "\tfold_size = int(len(dataset) / n_folds)\n", 
        "\tfor _ in range(n_folds):\n", 
        "\t\tfold = list()\n", 
        "\t\twhile len(fold) < fold_size:\n", 
        "\t\t\tindex = randrange(len(dataset_copy))\n", 
        "\t\t\tfold.append(dataset_copy.pop(index))\n", 
        "\t\tdataset_split.append(fold)\n", 
        "\treturn dataset_split\n", 
        "\n", 
        "# Calculate accuracy percentage\n", 
        "def accuracy_metric(actual, predicted):\n", 
        "\tcorrect = 0\n", 
        "\tfor i in range(len(actual)):\n", 
        "\t\tif actual[i] == predicted[i]:\n", 
        "\t\t\tcorrect += 1\n", 
        "\treturn correct / float(len(actual)) * 100.0\n", 
        "\n", 
        "# Evaluate an algorithm using a cross validation split\n", 
        "def evaluate_algorithm(dataset, algorithm, n_folds, *args):\n", 
        "\tfolds = cross_validation_split(dataset, n_folds)\n", 
        "\tscores = list()\n", 
        "\tfor fold in folds:\n", 
        "\t\ttrain_set = list(folds)\n", 
        "\t\ttrain_set.remove(fold)\n", 
        "\t\ttrain_set = sum(train_set, [])\n", 
        "\t\ttest_set = list()\n", 
        "\t\tfor row in fold:\n", 
        "\t\t\trow_copy = list(row)\n", 
        "\t\t\ttest_set.append(row_copy)\n", 
        "\t\t\trow_copy[-1] = None\n", 
        "\t\tpredicted = algorithm(train_set, test_set, *args)\n", 
        "\t\tactual = [row[-1] for row in fold]\n", 
        "\t\taccuracy = accuracy_metric(actual, predicted)\n", 
        "\t\tscores.append(accuracy)\n", 
        "\treturn scores\n", 
        "\n", 
        "# Make a prediction with weights\n", 
        "def predict(row, weights):\n", 
        "\tactivation = weights[0]\n", 
        "\tfor i in range(len(row)-1):\n", 
        "\t\tactivation += weights[i + 1] * row[i]\n", 
        "\treturn 1.0 if activation >= 0.0 else 0.0\n", 
        "\n", 
        "# Estimate Perceptron weights using stochastic gradient descent\n", 
        "def train_weights(train, l_rate, n_epoch):\n", 
        "\tweights = [0.0 for i in range(len(train[0]))]\n", 
        "\tfor _ in range(n_epoch):\n", 
        "\t\tfor row in train:\n", 
        "\t\t\tprediction = predict(row, weights)\n", 
        "\t\t\terror = row[-1] - prediction\n", 
        "\t\t\tweights[0] = weights[0] + l_rate * error\n", 
        "\t\t\tfor i in range(len(row)-1):\n", 
        "\t\t\t\tweights[i + 1] = weights[i + 1] + l_rate * error * row[i]\n", 
        "\treturn weights\n", 
        "\n", 
        "# Perceptron Algorithm With Stochastic Gradient Descent\n", 
        "def perceptron(train, test, l_rate, n_epoch):\n", 
        "\tpredictions = list()\n", 
        "\tweights = train_weights(train, l_rate, n_epoch)\n", 
        "\tfor row in test:\n", 
        "\t\tprediction = predict(row, weights)\n", 
        "\t\tpredictions.append(prediction)\n", 
        "\treturn(predictions)\n"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "We will use the predict() and train weights() functions created above to train the model\n", 
        "and a new perceptron() function to tie them together."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }, 
    {
      "source": [
        "# Test the Perceptron algorithm on the sonar dataset\n", 
        "seed(1)\n", 
        "# load and prepare data\n", 
        "filename = 'sonar.all-data.csv'\n", 
        "dataset = load_csv(filename)\n", 
        "for i in range(len(dataset[0])-1):\n", 
        "\tstr_column_to_float(dataset, i)\n", 
        "# convert string class to integers\n", 
        "str_column_to_int(dataset, len(dataset[0])-1)\n", 
        "# evaluate algorithm\n", 
        "n_folds = 3\n", 
        "l_rate = 0.01\n", 
        "n_epoch = 500\n", 
        "scores = evaluate_algorithm(dataset, perceptron, n_folds, l_rate, n_epoch)\n", 
        "print('Scores: %s' % scores)\n", 
        "print('Mean Accuracy: %.3f%%' % (sum(scores)/float(len(scores))))"
      ], 
      "cell_type": "code", 
      "execution_count": null, 
      "outputs": [], 
      "metadata": {}
    }, 
    {
      "source": [
        "A k value of 3 was used for cross-validation, giving each fold 208 3 = 69:3 or just under 70\n", 
        "records to be evaluated upon each iteration. A learning rate of 0.1 and 500 training epochs were\n", 
        "chosen with a little experimentation. You can try your own configurations and see if you can\n", 
        "beat my score.\n", 
        "Running this example prints the scores for each of the 3 cross-validation folds then prints\n", 
        "the mean classification accuracy. We can see that the accuracy is about 72%, higher than the\n", 
        "baseline value of just over 50%."
      ], 
      "cell_type": "markdown", 
      "metadata": {}
    }
  ], 
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3", 
      "name": "python3", 
      "language": "python"
    }, 
    "language_info": {
      "mimetype": "text/x-python", 
      "nbconvert_exporter": "python", 
      "name": "python", 
      "file_extension": ".py", 
      "version": "3.6.1", 
      "pygments_lexer": "ipython3", 
      "codemirror_mode": {
        "version": 3, 
        "name": "ipython"
      }
    }, 
    "anaconda-cloud": {}
  }
}